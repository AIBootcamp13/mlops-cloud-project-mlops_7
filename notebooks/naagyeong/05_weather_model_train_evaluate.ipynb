{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86372f1b965adb5",
   "metadata": {},
   "source": [
    "# Notebook 기본 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bbee03e090649db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant 선언\n",
    "\n",
    "# 프로젝트 루트 디렉토리를 식별하기 위한 마커 파일 이름\n",
    "ROOT_MARKER = \"pyproject.toml\"\n",
    "\n",
    "# 한글 표시를 위한 나눔바른고딕 폰트 파일 이름\n",
    "# matplotlib 의 font_manager 에 실제 폰트 파일의 위치를 넣어주어야 한다.\n",
    "KOREAN_FONT_FILE = \"NanumBarunGothic.ttf\"\n",
    "\n",
    "# matplotlib 에서는 font-family 의 이름으로 font 를 설정한다.\n",
    "# 그래서 font 파일 그 자체가 아니라, 그 파일의 family 이름을 적어준다.\n",
    "KOREAN_FONT_FAMILY = \"NanumBarunGothic\"\n",
    "\n",
    "# 참고\n",
    "# Font Family 와 Font File 의 차이는,\n",
    "# Font Family 는 비슷한 디자인 특성을 공유하는 글꼴 그룹을 의미한다.\n",
    "#\n",
    "# 예를 들어 '나눔바른고딕' 폰트 패밀리는 일반(Regular), 굵게(Bold), 기울임(Italic) 등 여러 스타일을 포함할 수 있다.\n",
    "# 반면, 폰트 파일(.ttf, .otf 등)은 이러한 폰트의 하나의 스타일이 저장된 실제 파일이다.\n",
    "#\n",
    "# 이 프로젝트에서는 폰트 용량을 줄이기 위해 일반(Regular) 인 NanumBarunGothic.ttf 만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f2746669fe3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트 root 를 sys.path 에 추가해서 import 구문을 사용하기 쉽게\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"\n",
    "    pyproject.toml 파일을 기준으로 루트 디렉토리를 찾는다.\n",
    "    :return: Path: 프로젝트 루트 디렉토리 경로\n",
    "    \"\"\"\n",
    "\n",
    "    current_path = Path().resolve()\n",
    "\n",
    "    while current_path != current_path.parent:\n",
    "        if (current_path / ROOT_MARKER).exists():\n",
    "            return current_path\n",
    "\n",
    "        current_path = current_path.parent\n",
    "\n",
    "    raise FileNotFoundError(\"프로젝트 루트 디렉토리를 찾을 수 없습니다.\")\n",
    "\n",
    "\n",
    "ROOT_DIR = find_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2102dd84f4bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib 의 한글 font 설정\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "FONTS_DATA_DIR = ROOT_DIR / \"notebooks\" / \"fonts\"\n",
    "\n",
    "\n",
    "def setup_korean_font():\n",
    "    font_path = FONTS_DATA_DIR / KOREAN_FONT_FILE\n",
    "    fm.fontManager.addfont(font_path)\n",
    "\n",
    "    # 폰트 설정\n",
    "    plt.rcParams[\"font.family\"] = KOREAN_FONT_FAMILY\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "\n",
    "setup_korean_font()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af453d",
   "metadata": {},
   "source": [
    "# ver.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725413a0",
   "metadata": {},
   "source": [
    "## (1) 코드흐름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e273da",
   "metadata": {},
   "source": [
    "[Step 1] 데이터 분할(split_data)\n",
    "- S3에서 feature 데이터 로딩\n",
    "- 학습/검증/테스트 셋으로 분할하여 JSON 반환\n",
    "\n",
    "[Step 2] 모델 훈련(train)\n",
    "- 모델 config 리스트 생성 (모델명, 파라미터)\n",
    "- 각 모델 학습\n",
    "- 학습된 모델 `.pkl`로 저장\n",
    "- config에 저장 경로 추가하여 리턴\n",
    "\n",
    "[Step 3] 모델 평가(evaluate)\n",
    "- 모든 모델을 검증셋(X_val, y_val)으로 평가\n",
    "- wandb에 metric log (val_f1, val_accuracy 등)\n",
    "- best_f1 기준으로 가장 좋은 모델 선정\n",
    "- best 모델 `.pkl` 경로와 run_name 리턴\n",
    "\n",
    "[Step 4] 테스트(test)\n",
    "- best 모델을 테스트셋(X_test, y_test)으로 평가\n",
    "- wandb에 test metric log\n",
    "- 실서비스 전 모델 최종 성능 검증\n",
    "\n",
    "[Step 5] 아티팩트 등록(register_artifacts)\n",
    "- 모든 모델 `.pkl`을 W&B Artifact로 업로드\n",
    "- type=\"model\", name=run_name 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261f8a4",
   "metadata": {},
   "source": [
    "## (2) 모델 분석결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fbfaab",
   "metadata": {},
   "source": [
    "- **목표**: 다양한 분류 모델의 성능을 비교하여 최적의 날씨 예측 모델 선정\n",
    "- **데이터셋**: 테스트셋 기준 평가\n",
    "- **지표**: `accuracy`, `f1`, `precision`, `recall`, `balanced_accuracy`\n",
    "\n",
    "### 평균 Test 성능 기준 Top 모델\n",
    "\n",
    "| 순위 | 모델 이름 | 평균 점수 |\n",
    "|------|-----------|------------|\n",
    "| 1 | DecisionTreeClassifier (max_depth=10) | **1.0000** |\n",
    "| 1 | DecisionTreeClassifier (max_depth=5) | **1.0000** |\n",
    "| 3 | XGBClassifier (n_estimators=200, max_depth=5) | 0.99998 |\n",
    "| 3 | XGBClassifier (n_estimators=100, max_depth=3) | 0.99998 |\n",
    "| 3 | GradientBoostingClassifier (n_estimators=200, learning_rate=0.05) | 0.99998 |\n",
    "| 3 | GradientBoostingClassifier (n_estimators=100, learning_rate=0.1) | 0.99998 |\n",
    "| 7 | RandomForestClassifier (n_estimators=200, max_depth=10) | 0.99974 |\n",
    "| 8 | RandomForestClassifier (n_estimators=100, max_depth=5) | 0.97087 |\n",
    "\n",
    "### 상대적으로 성능이 낮은 모델\n",
    "\n",
    "| 모델 이름 | 평균 점수 |\n",
    "|-----------|------------|\n",
    "| GaussianNB | 0.6852 |\n",
    "| KNeighborsClassifier (n_neighbors=5) | 0.4211 |\n",
    "| KNeighborsClassifier (n_neighbors=3) | 0.4135 |\n",
    "| LogisticRegression (C=1.0) | 0.3046 |\n",
    "| LogisticRegression (C=0.1) | 0.2898 |\n",
    "| LinearSVC (C=1.0 / 0.5) | 0.1675 |\n",
    "\n",
    "### 모델 선택 전략\n",
    "\n",
    "| 전략 | 설명 |\n",
    "|------|------|\n",
    "| **단일 모델 서빙** | `DecisionTreeClassifier (max_depth=10)` 추천. 해석 용이하고 예측 속도 빠름. |\n",
    "| **앙상블 모델** | XGB, GBDT, RF 모델 3~5개를 soft voting 또는 평균 확률 방식으로 앙상블 |\n",
    "| **FastAPI 연동용 추천** | 성능 + 추론 속도 + 라이브러리 안정성 고려 시 `XGBClassifier` 최우선 추천 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb7c3e",
   "metadata": {},
   "source": [
    "## (3) automated_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1478627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from config.default_args import get_dynamic_default_args\n",
    "from config.keys import KEY_FEATURE_DATASET_STORAGE_KEY\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.models import Variable\n",
    "from src.libs.storage import Storage\n",
    "from src.utils.log import get_logger\n",
    "\n",
    "\n",
    "default_args = get_dynamic_default_args()\n",
    "_logger = get_logger(\"weather_automated_pipeline\")\n",
    "\n",
    "\n",
    "@dag(\n",
    "    dag_id=\"weather_automated_pipeline\",\n",
    "    start_date=datetime(2025, 6, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    tags=[\"weather\", \"ml-modeling\"],\n",
    "    default_args=default_args,\n",
    ")\n",
    "def automated_pipeline_dag():\n",
    "    storage = Storage.create()\n",
    "\n",
    "    def get_model_configs():\n",
    "        model_registry = {\n",
    "            \"LogisticRegression\": LogisticRegression,\n",
    "            \"RandomForestClassifier\": RandomForestClassifier,\n",
    "            \"GradientBoostingClassifier\": GradientBoostingClassifier,\n",
    "            \"LinearSVC\": LinearSVC,\n",
    "            \"GaussianNB\": GaussianNB,\n",
    "            \"KNeighborsClassifier\": KNeighborsClassifier,\n",
    "            \"DecisionTreeClassifier\": DecisionTreeClassifier,\n",
    "            \"XGBClassifier\": XGBClassifier,\n",
    "        }\n",
    "\n",
    "        param_grid = {\n",
    "            \"LogisticRegression\": [\n",
    "                {\"C\": 0.1, \"max_iter\": 100},\n",
    "                {\"C\": 1.0, \"max_iter\": 200},\n",
    "            ],\n",
    "            \"RandomForestClassifier\": [\n",
    "                {\"n_estimators\": 100, \"max_depth\": 5},\n",
    "                {\"n_estimators\": 200, \"max_depth\": 10},\n",
    "            ],\n",
    "            \"GradientBoostingClassifier\": [\n",
    "                {\"n_estimators\": 100, \"learning_rate\": 0.1},\n",
    "                {\"n_estimators\": 200, \"learning_rate\": 0.05},\n",
    "            ],\n",
    "            \"LinearSVC\": [\n",
    "                {\"C\": 1.0, \"max_iter\": 1000},\n",
    "                {\"C\": 0.5, \"max_iter\": 1000},\n",
    "            ],\n",
    "            \"GaussianNB\": [{}],\n",
    "            \"KNeighborsClassifier\": [\n",
    "                {\"n_neighbors\": 3},\n",
    "                {\"n_neighbors\": 5},\n",
    "            ],\n",
    "            \"DecisionTreeClassifier\": [\n",
    "                {\"max_depth\": 5},\n",
    "                {\"max_depth\": 10},\n",
    "            ],\n",
    "            \"XGBClassifier\": [\n",
    "                {\"n_estimators\": 100, \"max_depth\": 3, \"use_label_encoder\": False, \"eval_metric\": \"mlogloss\"},\n",
    "                {\"n_estimators\": 200, \"max_depth\": 5, \"use_label_encoder\": False, \"eval_metric\": \"mlogloss\"},\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        config_list = []\n",
    "        for model_name, model_class in model_registry.items():\n",
    "            for param in param_grid[model_name]:\n",
    "                config_list.append({\"name\": model_name, \"model_class_str\": model_class.__name__, \"params\": param})\n",
    "\n",
    "        return config_list\n",
    "\n",
    "    def evaluate_model(model, X, y, prefix=\"val\"):\n",
    "        preds = model.predict(X)\n",
    "        return {\n",
    "            f\"{prefix}_accuracy\": accuracy_score(y, preds),\n",
    "            f\"{prefix}_f1\": f1_score(y, preds, average=\"macro\"),\n",
    "            f\"{prefix}_precision\": precision_score(y, preds, average=\"macro\", zero_division=0),\n",
    "            f\"{prefix}_recall\": recall_score(y, preds, average=\"macro\", zero_division=0),\n",
    "            f\"{prefix}_balanced_accuracy\": balanced_accuracy_score(y, preds),\n",
    "        }\n",
    "\n",
    "    @task\n",
    "    def split_data():\n",
    "        feature_storage_key = Variable.get(KEY_FEATURE_DATASET_STORAGE_KEY)\n",
    "        features = storage.read_as_dataframe(feature_storage_key)\n",
    "        X = features.drop(columns=[\"weather\"])\n",
    "        y = features[\"weather\"]\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "        return {\n",
    "            \"X_train\": X_train.to_json(orient=\"split\"),\n",
    "            \"y_train\": y_train.tolist(),\n",
    "            \"X_val\": X_val.to_json(orient=\"split\"),\n",
    "            \"y_val\": y_val.tolist(),\n",
    "            \"X_test\": X_test.to_json(orient=\"split\"),\n",
    "            \"y_test\": y_test.tolist(),\n",
    "        }\n",
    "\n",
    "    @task\n",
    "    def train(data_dict: dict):\n",
    "        configs = get_model_configs()\n",
    "        X_train = pd.read_json(StringIO(data_dict[\"X_train\"]), orient=\"split\")\n",
    "        y_train = pd.Series(data_dict[\"y_train\"])\n",
    "\n",
    "        for config in configs:\n",
    "            model_cls = eval(config[\"model_class_str\"])\n",
    "            model = model_cls(**config[\"params\"])\n",
    "            model.fit(X_train, y_train)\n",
    "            model_path = f\"{config['name']}_{str(config['params']).replace(' ', '').replace(':', '-')}.pkl\"\n",
    "            joblib.dump(model, model_path)\n",
    "            config[\"model_path\"] = model_path\n",
    "        return configs\n",
    "\n",
    "    @task\n",
    "    def evaluate(configs: list, data_dict: dict):\n",
    "        X_val = pd.read_json(StringIO(data_dict[\"X_val\"]), orient=\"split\")\n",
    "        y_val = pd.Series(data_dict[\"y_val\"])\n",
    "\n",
    "        best_model_path = \"\"\n",
    "        best_f1 = -1\n",
    "        best_run_name = \"\"\n",
    "\n",
    "        for config in configs:\n",
    "            model = joblib.load(config[\"model_path\"])\n",
    "            model_name = config[\"name\"]\n",
    "            params = config[\"params\"]\n",
    "            run_name = f\"{model_name}_{'_'.join([f'{k}={v}' for k, v in params.items()])}\".replace(\"=\", \"-\")\n",
    "\n",
    "            run = wandb.init(\n",
    "                project=\"weather-classification\", name=run_name, config={\"model\": model_name, **params}, reinit=True\n",
    "            )\n",
    "            val_metrics = evaluate_model(model, X_val, y_val)\n",
    "            wandb.log(val_metrics)\n",
    "\n",
    "            if val_metrics[\"val_f1\"] > best_f1:\n",
    "                best_f1 = val_metrics[\"val_f1\"]\n",
    "                best_model_path = config[\"model_path\"]\n",
    "                best_run_name = run_name\n",
    "\n",
    "            wandb.save(config[\"model_path\"])\n",
    "            wandb.finish()\n",
    "\n",
    "        return {\"best_model_path\": best_model_path, \"best_run_name\": best_run_name, \"all_configs\": configs}\n",
    "\n",
    "    @task\n",
    "    def test(meta: dict, data_dict: dict):\n",
    "        X_test = pd.read_json(StringIO(data_dict[\"X_test\"]), orient=\"split\")\n",
    "        y_test = pd.Series(data_dict[\"y_test\"])\n",
    "\n",
    "        for config in meta[\"all_configs\"]:\n",
    "            model = joblib.load(config[\"model_path\"])\n",
    "            model_name = config[\"name\"]\n",
    "            params = config[\"params\"]\n",
    "            run_name = f\"{model_name}_{'_'.join([f'{k}={v}' for k, v in params.items()])}\".replace(\"=\", \"-\") + \"_test\"\n",
    "\n",
    "            run = wandb.init(project=\"weather-classification\", name=run_name, job_type=\"test-eval\", reinit=True)\n",
    "            test_metrics = evaluate_model(model, X_test, y_test, prefix=\"test\")\n",
    "            wandb.log(test_metrics)\n",
    "            wandb.finish()\n",
    "\n",
    "    @task\n",
    "    def register_artifacts(meta: dict):\n",
    "        for config in meta[\"all_configs\"]:\n",
    "            run_name = f\"{config['name']}_{'_'.join([f'{k}={v}' for k, v in config['params'].items()])}\".replace(\n",
    "                \"=\", \"-\"\n",
    "            )\n",
    "            artifact = wandb.Artifact(run_name, type=\"model\")\n",
    "            artifact.add_file(config[\"model_path\"])\n",
    "            run = wandb.init(\n",
    "                project=\"weather-classification\", name=f\"{run_name}_artifact\", job_type=\"register\", reinit=True\n",
    "            )\n",
    "            wandb.log_artifact(artifact)\n",
    "            run.finish()\n",
    "\n",
    "    data = split_data()\n",
    "    model_info = train(data)\n",
    "    eval_meta = evaluate(model_info, data)\n",
    "    test(eval_meta, data)\n",
    "    register_artifacts(eval_meta)\n",
    "\n",
    "\n",
    "automated_pipeline_dag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56170147",
   "metadata": {},
   "source": [
    "# ver.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ade25",
   "metadata": {},
   "source": [
    "## (1) 코드흐름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a784b3",
   "metadata": {},
   "source": [
    "[Step 1] 데이터 분할 (`split_data`)\n",
    "- S3에서 feature 데이터 로딩 (`Storage.read_as_dataframe`)\n",
    "- `weather` 컬럼을 타겟으로 학습/검증/테스트 셋 분할  \n",
    "  - Train: 60%, Validation: 20%, Test: 20%\n",
    "- 각 셋을 JSON 형식으로 반환\n",
    "\n",
    "[Step 2] 모델 훈련 (`train`)\n",
    "- 사용 모델:\n",
    "  - `DecisionTreeClassifier(max_depth=10)`\n",
    "  - `XGBClassifier`\n",
    "  - `Soft Voting 앙상블` (XGB + GBDT + RF)\n",
    "- 각 모델 학습\n",
    "- 학습된 모델을 `.pkl`로 저장하고, config에 경로 추가하여 리턴\n",
    "\n",
    "[Step 3] 모델 평가 (`evaluate`)\n",
    "- 모든 모델을 검증셋 `(X_val, y_val)`으로 평가\n",
    "- 주요 평가 지표:\n",
    "  - `val_accuracy`, `val_f1`, `val_precision`, `val_recall`, `val_balanced_accuracy`\n",
    "- 각 모델의 metric을 wandb에 기록\n",
    "- `val_f1` 기준으로 가장 좋은 모델을 선정하여 `.pkl` 경로와 `run_name` 리턴\n",
    "\n",
    "[Step 4] 테스트 (`test`)\n",
    "- 선택된 모든 모델을 테스트셋 `(X_test, y_test)`으로 평가\n",
    "- 주요 평가 지표:\n",
    "  - `test_accuracy`, `test_f1`, `test_precision`, `test_recall`, `test_balanced_accuracy`\n",
    "- wandb에 각 모델의 test metric 기록  \n",
    "- 실제 배포 전 최종 성능 검증 단계\n",
    "\n",
    "[Step 5] 아티팩트 등록 (`register_artifacts`)\n",
    "- 모든 모델 `.pkl` 파일을 W&B Artifact로 업로드\n",
    "  - Artifact 타입: `\"model\"`\n",
    "  - 이름: 각 모델의 `run_name` 기준\n",
    "- 이후 FastAPI API 서버 등에서 `wandb.Artifact`를 통해 불러올 수 있도록 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941d823",
   "metadata": {},
   "source": [
    "## (2) 모델 분석 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cba61b",
   "metadata": {},
   "source": [
    "| 모델 | val_f1 | test_f1 | 특징 |\n",
    "|------|--------|---------|------|\n",
    "| `DecisionTreeClassifier(max_depth=10)` | 1.000 | 1.000 | 단일 모델, 해석 용이 |\n",
    "| `XGBClassifier` | 1.000 | 1.000 | 성능 우수, FastAPI 적합 |\n",
    "| `Soft Voting 앙상블` | 1.000 | 0.99998 | 성능 유사, 추론 시간 약간 증가 |\n",
    "\n",
    "\n",
    "> ⚠ **Note**: 모든 모델이 `f1_score=1.0`에 가까움 → 데이터 누수 또는 클래스 불균형 여부 추가 검토 필요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d0675",
   "metadata": {},
   "source": [
    "## (3) automated_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defcc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from config.default_args import KEY_FEATURE_DATASET_STORAGE_KEY, get_dynamic_default_args\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from airflow.decorators import dag\n",
    "from src.utils.log import get_logger\n",
    "\n",
    "\n",
    "default_args = get_dynamic_default_args()\n",
    "_logger = get_logger(\"weather_automated_pipeline\")\n",
    "\n",
    "\n",
    "@dag(\n",
    "    dag_id=\"weather_automated_pipeline\",\n",
    "    start_date=datetime(2025, 6, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    tags=[\"weather\", \"ml-modeling\"],\n",
    "    default_args=default_args,\n",
    ")\n",
    "def automated_pipeline_dag():\n",
    "    storage = Storage.create()\n",
    "\n",
    "    def evaluate_model(model, X, y, prefix=\"val\"):\n",
    "        preds = model.predict(X)\n",
    "        return {\n",
    "            f\"{prefix}_accuracy\": accuracy_score(y, preds),\n",
    "            f\"{prefix}_f1\": f1_score(y, preds, average=\"macro\"),\n",
    "            f\"{prefix}_precision\": precision_score(y, preds, average=\"macro\", zero_division=0),\n",
    "            f\"{prefix}_recall\": recall_score(y, preds, average=\"macro\", zero_division=0),\n",
    "            f\"{prefix}_balanced_accuracy\": balanced_accuracy_score(y, preds),\n",
    "        }\n",
    "\n",
    "    @task\n",
    "    def split_data():\n",
    "        feature_storage_key = Variable.get(KEY_FEATURE_DATASET_STORAGE_KEY)\n",
    "        features = storage.read_as_dataframe(feature_storage_key)\n",
    "        X = features.drop(columns=[\"weather\"])\n",
    "        y = features[\"weather\"]\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "        return {\n",
    "            \"X_train\": X_train.to_json(orient=\"split\"),\n",
    "            \"y_train\": y_train.tolist(),\n",
    "            \"X_val\": X_val.to_json(orient=\"split\"),\n",
    "            \"y_val\": y_val.tolist(),\n",
    "            \"X_test\": X_test.to_json(orient=\"split\"),\n",
    "            \"y_test\": y_test.tolist(),\n",
    "        }\n",
    "\n",
    "    @task\n",
    "    def train(data_dict: dict):\n",
    "        X_train = pd.read_json(StringIO(data_dict[\"X_train\"]), orient=\"split\")\n",
    "        y_train = pd.Series(data_dict[\"y_train\"])\n",
    "\n",
    "        models = {\n",
    "            \"decision_tree\": DecisionTreeClassifier(max_depth=10),\n",
    "            \"xgb_classifier\": XGBClassifier(\n",
    "                n_estimators=200, max_depth=5, use_label_encoder=False, eval_metric=\"mlogloss\"\n",
    "            ),\n",
    "            \"ensemble_soft\": VotingClassifier(\n",
    "                estimators=[\n",
    "                    (\n",
    "                        \"xgb\",\n",
    "                        XGBClassifier(n_estimators=100, max_depth=3, use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "                    ),\n",
    "                    (\"rf\", RandomForestClassifier(n_estimators=100, max_depth=5)),\n",
    "                    (\"gbdt\", GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)),\n",
    "                ],\n",
    "                voting=\"soft\",\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        model_paths = {}\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            path = f\"{name}.pkl\"\n",
    "            joblib.dump(model, path)\n",
    "            model_paths[name] = path\n",
    "        return model_paths\n",
    "\n",
    "    @task\n",
    "    def evaluate(model_paths: dict, data_dict: dict):\n",
    "        X_val = pd.read_json(StringIO(data_dict[\"X_val\"]), orient=\"split\")\n",
    "        y_val = pd.Series(data_dict[\"y_val\"])\n",
    "        X_test = pd.read_json(StringIO(data_dict[\"X_test\"]), orient=\"split\")\n",
    "        y_test = pd.Series(data_dict[\"y_test\"])\n",
    "\n",
    "        best_model_name = \"\"\n",
    "        best_model_path = \"\"\n",
    "        best_f1 = -1\n",
    "\n",
    "        for name, path in model_paths.items():\n",
    "            model = joblib.load(path)\n",
    "\n",
    "            run = wandb.init(project=\"weather-classification\", name=name, config={\"model\": name}, reinit=True)\n",
    "\n",
    "            val_metrics = evaluate_model(model, X_val, y_val, prefix=\"val\")\n",
    "            test_metrics = evaluate_model(model, X_test, y_test, prefix=\"test\")\n",
    "\n",
    "            wandb.log({**val_metrics, **test_metrics})\n",
    "            wandb.save(path)\n",
    "            wandb.finish()\n",
    "\n",
    "            if val_metrics[\"val_f1\"] > best_f1:\n",
    "                best_f1 = val_metrics[\"val_f1\"]\n",
    "                best_model_name = name\n",
    "                best_model_path = path\n",
    "\n",
    "        return {\"name\": best_model_name, \"path\": best_model_path}\n",
    "\n",
    "    @task\n",
    "    def register_best_artifact(meta: dict):\n",
    "        artifact = wandb.Artifact(meta[\"name\"], type=\"model\")\n",
    "        artifact.add_file(meta[\"path\"])\n",
    "        run = wandb.init(\n",
    "            project=\"weather-classification\", name=f\"{meta['name']}_artifact\", job_type=\"register\", reinit=True\n",
    "        )\n",
    "        wandb.log_artifact(artifact)\n",
    "        run.finish()\n",
    "\n",
    "    data = split_data()\n",
    "    model_paths = train(data)\n",
    "    best_meta = evaluate(model_paths, data)\n",
    "    register_best_artifact(best_meta)\n",
    "\n",
    "\n",
    "automated_pipeline_dag()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
